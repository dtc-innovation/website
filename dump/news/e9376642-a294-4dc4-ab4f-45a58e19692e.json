{
  "activities": [
    "1f5f0ba7-d1a2-44c6-97e2-23990f691095"
  ],
  "content": {
    "en": "After a couple months delay due to the many parallel projects running, the <a href=\"http://www.medialab.sciences-po.fr/projets/dime-shs/\">DIME/Web team at the medialab</a> is finally proud to present the very first release or the latest tool we have been working on: <a href=\"https://github.com/medialab/Hypertext-Corpus-Initiative\">Hyphe</a>.\r\n\r\nAs the <a href=\"http://www.e-diasporas.fr/\">e-Diasporas project</a> illustrates, working on controversies can be greatly facilitated by applying the Actors/Network Theory and study the connections between the different actors and categories involved in a controversy. Crawling the websites of these actors and other linked websites in order to constitute a network of the hyperlinks between these actors provides a rich source of information to lead exploratory analysis and confirm hypothesis on the actors relationships.\r\n\r\nMany tools exist to mine the web, among which two we already use at the m√©dialab: the <a href=\"http://webatlas.fr/wp/navicrawler/\">Navicrawler</a> created by Mathieu Jacomy and the <a href=\"http://issuecrawler.net/\">Issue Crawler</a> from our partner <a href=\"http://digitalmethods.net/\">Digital Method Initiative</a> in Amsterdam. The existing Web archiving initiatives (in France <a href=\"http://www.bnf.fr/en/collections_and_services/book_press_media/a.internet_archives.html\">BnF</a> and <a href=\"http://www.inatheque.fr/fonds-audiovisuels/sites-web-media.html\">INA</a>) also show the interest one can have about web data. The Navicrawler allows on one side to build a web corpus step by step while browsing (filtering and qualifying websites to include in the corpus) and then export data to create a map based on link topology. On the other side, the Issue Crawler is a great tool to harvest a vast quantity of data automatically by crawling in \"snowball\" from site to site and filtering automatically sites to be included on the basis of a quantitative analysis. Those two approaches are complementary but could not be easily mixed up until now.\r\n\r\nHyphe was conceived to address both these issues : it was based on the idea that researchers need to control how the corpus is built by ensuring themselves the qualitative decisions such as qualifying or grouping, but also need to be equipped with powerful tools capable of handling the huge amount of data available on the web. The goal is to allow both qualitative (selection and qualification) and quantitative (data harvesting, indexing and storage) work thanks to this new tool designed for non-technical users among which social sciences researchers and librarians.\r\n\r\nHyphe relies therefore on three central principles :\r\n- Explore only within a list of desired websites, whereas other websites are only being discovered until a human decision is being taken to crawl them as well;\r\n- Define as precisely as possible what a website, or rather what we call a \"Web Entity\", really is, from a simple webpage to a combination of multiple domain names, including of course a whole website, or a combination of a Twitter webpage, a Wikipedia webpage and a subpart of a domain;\r\n- Easily browse, classify and qualify the corpus while constituting it from a web interface.\r\n\r\n<em></em>Using modern and robust technologies such as <a href=\"http://lucene.apache.org/\">Lucene</a>, <a href=\"http://www.mongodb.org/\">MongoDB</a>, <a href=\"http://scrapy.org/\">Scrapy</a>, <a href=\"http://twistedmatrix.com/trac/\">Twisted</a>, <a href=\"http://thrift.apache.org/\">Thrift</a>, <a href=\"http://dominojs.org/\">Domino.js</a>, <a href=\"http://sigmajs.org/\">Sigma.js</a> or <a href=\"http://getbootstrap.com/\">Bootstrap</a>, Hyphe's current release is only a sneakpeek preview of what it should soon be able to accomplish. The roadmap now includes features such as managing multiple corpora within each instance, bypassing crawling issues (redirections, cookies, javascript-only pages, ...), handling multi-websites entities from the web interface, tagging the results, and so on...\r\n\r\nAfter trying it out already on a couple of projects (<a href=\"http://www.medialab.sciences-po.fr/projets/emaps/\">EMAPS</a>, C-Section WHO, <a href=\"http://www.sciencespo.fr/bibliotheque/statique/elections-2012/index.php\">SITPOL</a>...) and spending much time adapting our original thoughts and implementations to the actual usages, the current alpha version of Hyphe is now releasable for anyone to start trying it out and using it individually for science.\r\n\r\n<em></em>This first release was made to be easily installable on an Ubuntu machine. Future versions will be prepared for a broader scope of configurations and our first experiences on MacOSX, Debian and CentOS were already encouraging.\r\nYou can <a href=\"https://github.com/medialab/Hypertext-Corpus-Initiative/releases/download/v0.0.0/hyphe-v0.0.0.tar.gz\">download the package (tar.gz archive) here</a> and <a href=\"https://github.com/medialab/Hypertext-Corpus-Initiative/releases/tag/v0.0.0\">read on GitHub the instructions to install and run it on Ubuntu</a>.\r\nReleased as a free software, the whole <a href=\"https://github.com/medialab/Hypertext-Corpus-Initiative\">sourcecode is also available on GitHub</a> under the Free OpenSource Software licences <span style=\"color: #000000;\"><a href=\"http://www.gnu.org/licenses/lgpl-3.0.en.html\">LGPL</a>/<a href=\"http://www.cecill.info/licences/Licence_CeCILL-C_V1-en.html\">Cecil C</a>,</span> so that advanced users or developers can contribute and redistribute it.\r\n\r\n<a href=\"http://hyphe.medialab.sciences-po.fr/demo/\" target=\"_blank\">A limited demo is available online here</a> so that anyone can try out Hyphe's functionnalities. Since this is still a one-corpus-only instance, anyone will be able to reset the data and crawls or overload them with undesired data, so remember this is only meant for tryouts and you should rather try and install your own instance for serious work use. Please do not hesitate and <a href=\"https://github.com/medialab/Hypertext-Corpus-Initiative/issues?state=open\">let us know on GitHub's issues</a> about any problem you would encounter while installing, running or even improving Hyphe.\r\n\r\nLong live Hyphe with many more releases to come!",
    "fr": ""
  },
  "description": {
    "en": "Hyphe, our new webcrawler, allows non-technical users, among which social sciences researchers and librarians, to control precisely how their web corpus is built. The main feature is to permit both qualitative (selection and qualification) and quantitative (data harvesting, indexing and storage) work on a huge amount of data.",
    "fr": ""
  },
  "draft": true,
  "id": "e9376642-a294-4dc4-ab4f-45a58e19692e",
  "internal": false,
  "lastUpdated": 1384677240000,
  "oldSlug": "hyphe-v0-0-0-the-first-release-of-our-new-webcrawler-is-out",
  "people": [
    "0dcba674-00f3-42d8-b943-89b698a29081",
    "910c509c-ca11-4684-9898-f2b0b6f3283e"
  ],
  "productions": [
    "19cf7adc-1880-4072-bfdf-c69eac7e6ffa"
  ],
  "slugs": [
    ""
  ],
  "startDate": "2013-11-17",
  "title": {
    "en": "Hyphe v0.0.0 : the first release of our new webcrawler is out !",
    "fr": ""
  },
  "type": "notice"
}