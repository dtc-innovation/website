{
  "activities": [
  ],
  "content": {
    "en": "BARCAMP e-diaspora Jeudi 24 Mai\r\n\r\nSynthèse de la journée BarCamp\r\n\r\n1- LE TEMPS\r\n\r\n# tracker les évolutions d'un corpus une fois constitué\r\n\r\nl'outils de création de corpus devrait être capable de le suivre dans le temps\r\n\r\nce n'est pas redondant avec l'archivage\r\n\r\npermet de suivre des évolutions prévisibles suite à des évènements : élections 2014\r\n\r\nsuivre des phénomènes d'extension ou concentration\r\n\r\n# remonter dans le temps : archivage\r\n\r\npolitique de sélection de l'archivage INA : premier sourcing prospectif réalisé par Linkfluence + mises à jour manuelles par les documentalistes\r\n\r\narchive permet de rejouer les sites (naviguer dans le temps)\r\n\r\ncomplémentarité des archives : Memento Web IIPC plugin firefox transaction, si il trouve la page dans différentes archives. a part annuaire communs : très difficiles techniquement.\r\n\r\n-> accepter les contraintes de l'archive : interopérabilité : très difficile, malgré la publication méta-données gros problème technique\r\n\r\nLes archives ont d'abord un objectif patrimoniale.\r\n\r\nElles peuvent être rejouées, on peut voir le web tel qu'il était au moment de la capture.\r\n\r\nmais il y a des limites à la consultation = web n'est pas fait pour être archivé\r\n\r\n-> consultation il reste des problèmes d'interprétation : parfois la reconstruction est difficile à appréhender, incohérences difficile à comprendre\r\n\r\nles archives sont consultables par des robots : par un proxy acceptant comme paramètre pour la date à consulter\r\n\r\n-> le futur outils HCI permettra de créer des corpus à partir des archives existantes (au moins celle de l'INA donc l'archive e-diaspora)\r\n\r\nqqn éléments à propos de l'archive e-diaspora :\r\n\r\narchive e-diaspora : 1 To 1 milliard d'URL, 1 fois par semaine ou 1 fois par mois\r\n\r\nlégalement : archive appartient à e-diaspora, publication des méta-données possibles\r\n\r\nusage : pour le moment aucun archivage fait en vue de e-diaspora 2\r\n\r\n-> la taille plus raisonnables des ces corpus peuvent nous laisser imaginer de réaliser du data mining au delà du moteur de recherche full text mis en place à l'INA\r\n\r\n# analyser dans le temps\r\n\r\nAnalyser l'évolution d'un corpus web dans le temps n'est pas du tout balisé.\r\n\r\nLes outils et les méthodes sont encore très fraiches mais activement développées que ce soit dans le domaine de la visualisation, du data mining ou du web/media studies.\r\n\r\n- représenter les évolutions : phylogénie en 3D\r\n\r\n- analyse dans le temps : normalisation sur échantillonnage variable, espoir du coté théorie des réseaux détection d'évènement dans des séries temporelles sans modèle\r\n\r\n2- L'ESPACE\r\n\r\n#  comment géolocaliser un site web ? (déjà traité lors d'un précédent workshop e-diaspora)\r\n\r\nVoici les 4 méthodes pour géolocaliser un site web :\r\n\r\n- où habitent les rédacteurs ?\r\n\r\n- où est hébergé le site ?\r\n\r\n- quels lieux géographiques sont cités ?\r\n\r\n- localisation administrative\r\n\r\ndans le cas de passage à la géographie, il est important de préciser quelle méthode à été utilisée car aucune d'elle n'est plus évidente ou plus valides que les autres. Elles apportent toutes des considérations différentes.\r\n\r\n# webifier la géographie : à l'inverse comment mobiliser la cartographie pour représenter les liens qu'entretient un corpus web avec le territoire réel\r\n\r\nquelques pistes à expérimenter :\r\n\r\n- remettre en cause la surface des pays comme indicateur de taille de la représentation\r\n\r\n- tester donc des cartogrammes en utilisant des données venant du corpus web\r\n\r\n- attention comme toujours à choisir une projection pertinente\r\n\r\n- un des enjeux consiste à ne pas chercher à niveler les différents niveaux administratif (villes, régions, pays)\r\n\r\nSi l'on s'intéresse aux noms de lieux cités dans un site web, laisser les énonciations telles qu'elles et représenter les différents niveau indépendamment en fonction de leur importance (nombre d'occurrence). (le service de nom de lieux comme geonames.org a été décrit)\r\n\r\n- le cas des lieux imaginaires ou mythologiques (May Houa) : C.F. wikipedia atlantis \"category: mythological places\"\r\n\r\n3- CONTENUS : au delà du lien\r\n\r\nComment dépasser la seule analyse topologique du graphe des liens hypertextes ?\r\n\r\nComment y ajouter des couches d'analyses de contenus : textes, images, méta-données (sur les sites, pages ou liens), données structurées venant de réseaux sociaux…\r\n\r\n# les liens hypertexte forment le squelette du corpus :\r\n\r\nle logiciel HCI permettra de créer et stocker, stabiliser un corpus.\r\n\r\nEn plus des liens ce logiciel captera les contenus textes par défaut.\r\n\r\nL'agrégation qu'il permettra des pages au niveau web entités (voir méthode) offrira la possibilité de récupérer et analyser les contenus textes.\r\n\r\nOn pourra alors imaginer ajouter à l'analyse réseau hypertextes, une analyse sémantique.\r\n\r\nDe la même manière à moyen terme on pourrait imaginer ajouter des couches d'extraction puis traitement de contenus spécifiques :\r\n\r\n- extraction et reconnaissance d'images\r\n\r\n- extraction de twitter\r\n\r\n- extraction de données Facebook\r\n\r\nCes extraction pourront être traitées dans la même structure mémoire en tant qu'entités propre (non web). On pourra alors imaginer déclarer des liens entre entités :\r\n\r\n- lien entre une image et toutes les pages web qui la contiennent\r\n\r\n- lien entre une page web et tous les tweet qui la citent\r\n\r\n- lien entre un profile facebook et toutes les pages qu'il a liké…\r\n\r\nCe travail demandera la programmation de moteur d'extraction spéficique en fonction des plateformes. Pour un futur à moyen terme.\r\n\r\n4- METHODES : exploiter les degrés de libertés pour répondre à une question de recherche\r\n\r\n# interactions avec les terrains classiques\r\n\r\ndans les deux sens :\r\n\r\nweb vers terrain : utiliser son corpus web pour préparer une étude de terrain, retourner sur le terrain pour confronter le miroir déformant qu'est le web à la réalité des acteurs\r\n\r\nterrain vers web : compléter une étude (quanti ou quali) par une étude web de la même question\r\n\r\ncomplémentaire que si on sait \"faire avec\" le web. N'oubions pas que le web existe !\r\n\r\nexemple : il existe les groupes sociaux qui souhaitent rester invisibles sur le web (pour protection de la vie privée Facebook ou pour garantir la clandestinité).\r\n\r\nPour ces cas il faut avoir recours à la méthode bien connue des ethnologues : l'observation participative !\r\n\r\nC'est en prenant par au groupe que vous pourrez l'observer.\r\n\r\n# granularité : logique de site contre logique de pages\r\n\r\nl'outils HCI est basé sur ce principe. Chaque chercheur pourra définir la juste granulité à utiliser pour chaque entité web qu'il souhaite étudier (voir : http://jiminy.medialab.sciences-po.fr/hci/index.php/Web_entities)\r\n\r\nOn notera qu'il est théoriquement possible de créer des règles d'agrégation différentes sur un même corpus de pages web récoltés afin de permettre à différent chercheurs d'agréger/sélectionner les entités en fonction de leur question de recherche (voir http://cartonomics.org/2012/05/27/de-la-replicabilite-des-corpus-de-sites-web/)\r\n\r\n# sélection : comment sélectionner des points d'entrées, étendre le corpus et éviter l'aspiration ver la couche haute\r\n\r\n2 chose sont indispensables à assurer de bons critères de sélections des entités web à inclure ou exclure au corpus :\r\n\r\n- ré-expliciter la méthode et assurer un accompagnement des chercheurs à ce travail qui demande de bien comprendre la structure du web (service fourni par l'équipement d'excellence DIME-SHS/web)\r\n\r\n- une meilleur interaction entre décision du chercheur et l'automatisme du crawler (problème réglé par HCI) (voir sur ce potin le résumé écrit par JC Plantin de la session \"analyse comparée http://cartonomics.org/2012/05/27/de-la-replicabilite-des-corpus-de-sites-web/)\r\n\r\n# Open the black box\r\n\r\nComme pour toute méthode, il est indispensable que les chercheurs comprennent les enjeux des données.\r\n\r\nLes logiciels que nous créer doivent donc :\r\n\r\n- être open source pour garantir la lecture critique du code produit (voir doi:10.1038/nature10836 http://www.nature.com/nature/journal/v482/n7386/full/nature10836.html)\r\n\r\n- toujours donner à voir aux chercheurs les conséquences de ses décisions en terme de \"qualité\" des données\r\n\r\n- assurer un historique aussi complet que possible qui permettent de retracer la chaîne de décisions/traitements appliqués aux données\r\n\r\n# méthode exploratoire : \"to help the researcher construct the research question\"\r\n\r\nLa méthode de construction et d'analyse de corpus web est une méthode exploratoire.\r\n\r\nElle permet en grande partie de construire ou plutôt de faire évoluer une question de recherche.\r\n\r\nElle demande plusieurs itérations avant d'aboutir à un corpus qui soit adapté au positionnement du chercheur.\r\n\r\nLa classification set un bon exemple de ce fait. Elle est construite en aller retour entre récolte et analyse.\r\n\r\nDes efforts sont à mener pour permettre à un groupe de chercheur de comparer leurs corpus sur la base d'éléments de classification commun.\r\n\r\nCet enjeux dépasse le cadre restraint de la méthode web.\r\n\r\nNotre approche privilégiée au médialab serait de mêler le bottom-up au top-down en permettant au chercheur de créer librement sa grille de qualification tout en y incluant des autorités de vocabulaire contrôlé.\r\n\r\nCe serait un bon premier pas vers la comparabilité des corpus web (à ce sujet voir http://cartonomics.org/2012/05/27/de-la-replicabilite-des-corpus-de-sites-web/)\r\n\r\n# scalabilité : aller-retour avec de grand corpus non thématiques\r\n\r\nL'intention de départ qui a motivée la création d'outils de web mining spécialisé pour les chercheurs en Sciences sociales, mettait la priorité à la qualité devant la quantité.\r\n\r\nNous ciblons des corpus de petites tailles (relativement à l'échelle du web).\r\n\r\nCela étant Anat Ben-David a soulevé l'intérêt que pourrait représenter des corpus de plus grandes échelles qui permettraient de donner un contexte au corpus raffinés par le chercheur.\r\n\r\nDeux enjeux se cachent derrière cette intention :\r\n\r\n- Qualité et robustesse d'un outils capable de passer à l'échelle (passage de l'expérimentation à la création de logiciels professionnels)\r\n\r\n- de même les critères de sélection de cette plus grande échelle reste à être déterminés (follow the medium, TLD, en utilisant les index de grand moteur de recherche ?)\r\n\r\nLe logiciel HCI permettra de régler le niveau du compromis entre qualité et quantité.\r\n\r\nOn pourra choisir de récolter un grand nombre d'urls et les liens qu'ils échangent à condition de limiter la profondeur des détails au sein de ses entités ou bien se limiter dans le nombre d'URL afin de permettre une grande qualité de détails dans chacune d'elles. On pourra même mettre des exceptions de précisions qui permettront de sortir une page (ou un ensemble) précise comme une entité très profondément d'une entité web non détaillée suivant la règle classique (voir : http://jiminy.medialab.sciences-po.fr/hci/index.php/Precision_limit)\r\n\r\n# analyse visuelle : explorer, cartographier des questions\r\n\r\nL'analyse visuelle des corpus web est également un point méthodologique en développement.\r\n\r\nDeux difficultés :\r\n\r\n- les sciences sociales n'ont pas la tradition de l'image scientifique comme l'ont développée les sciences naturelles\r\n\r\n- exception faite de la raison graphique de Bertin qu'il faut ré-interpréter à l'heure des réseaux, de la dynamique de l'intéraction/exploration\r\n\r\nIl s'agit de donner les moyens au chercheur d'explorer (GEPHI) puis de cartographier (TubeMyNet en cours développement) son corpus.\r\n\r\nIl est important d'abandonner l'idée qu'une carte suffira à synthétiser le travail effectué.\r\n\r\nLes résultats d'un travail sur corpus web doivent retracer la constitution, l'exploration et l'analyse du corpus sous ces différentes facettes.\r\n\r\nCe n'est pas une carte mais bien des cartes, des récits, des visualisations qui pourront former l'ensemble suffisant à rendre compte du cheminement du chercheur.",
    "fr": "BARCAMP e-diaspora Jeudi 24 Mai\r\n\r\nSynthèse de la journée BarCamp\r\n\r\n1- LE TEMPS\r\n\r\n# tracker les évolutions d'un corpus une fois constitué\r\n\r\nl'outils de création de corpus devrait être capable de le suivre dans le temps\r\n\r\nce n'est pas redondant avec l'archivage\r\n\r\npermet de suivre des évolutions prévisibles suite à des évènements : élections 2014\r\n\r\nsuivre des phénomènes d'extension ou concentration\r\n\r\n# remonter dans le temps : archivage\r\n\r\npolitique de sélection de l'archivage INA : premier sourcing prospectif réalisé par Linkfluence + mises à jour manuelles par les documentalistes\r\n\r\narchive permet de rejouer les sites (naviguer dans le temps)\r\n\r\ncomplémentarité des archives : Memento Web IIPC plugin firefox transaction, si il trouve la page dans différentes archives. a part annuaire communs : très difficiles techniquement.\r\n\r\n-> accepter les contraintes de l'archive : interopérabilité : très difficile, malgré la publication méta-données gros problème technique\r\n\r\nLes archives ont d'abord un objectif patrimoniale.\r\n\r\nElles peuvent être rejouées, on peut voir le web tel qu'il était au moment de la capture.\r\n\r\nmais il y a des limites à la consultation = web n'est pas fait pour être archivé\r\n\r\n-> consultation il reste des problèmes d'interprétation : parfois la reconstruction est difficile à appréhender, incohérences difficile à comprendre\r\n\r\nles archives sont consultables par des robots : par un proxy acceptant comme paramètre pour la date à consulter\r\n\r\n-> le futur outils HCI permettra de créer des corpus à partir des archives existantes (au moins celle de l'INA donc l'archive e-diaspora)\r\n\r\nqqn éléments à propos de l'archive e-diaspora :\r\n\r\narchive e-diaspora : 1 To 1 milliard d'URL, 1 fois par semaine ou 1 fois par mois\r\n\r\nlégalement : archive appartient à e-diaspora, publication des méta-données possibles\r\n\r\nusage : pour le moment aucun archivage fait en vue de e-diaspora 2\r\n\r\n-> la taille plus raisonnables des ces corpus peuvent nous laisser imaginer de réaliser du data mining au delà du moteur de recherche full text mis en place à l'INA\r\n\r\n# analyser dans le temps\r\n\r\nAnalyser l'évolution d'un corpus web dans le temps n'est pas du tout balisé.\r\n\r\nLes outils et les méthodes sont encore très fraiches mais activement développées que ce soit dans le domaine de la visualisation, du data mining ou du web/media studies.\r\n\r\n- représenter les évolutions : phylogénie en 3D\r\n\r\n- analyse dans le temps : normalisation sur échantillonnage variable, espoir du coté théorie des réseaux détection d'évènement dans des séries temporelles sans modèle\r\n\r\n2- L'ESPACE\r\n\r\n#  comment géolocaliser un site web ? (déjà traité lors d'un précédent workshop e-diaspora)\r\n\r\nVoici les 4 méthodes pour géolocaliser un site web :\r\n\r\n- où habitent les rédacteurs ?\r\n\r\n- où est hébergé le site ?\r\n\r\n- quels lieux géographiques sont cités ?\r\n\r\n- localisation administrative\r\n\r\ndans le cas de passage à la géographie, il est important de préciser quelle méthode à été utilisée car aucune d'elle n'est plus évidente ou plus valides que les autres. Elles apportent toutes des considérations différentes.\r\n\r\n# webifier la géographie : à l'inverse comment mobiliser la cartographie pour représenter les liens qu'entretient un corpus web avec le territoire réel\r\n\r\nquelques pistes à expérimenter :\r\n\r\n- remettre en cause la surface des pays comme indicateur de taille de la représentation\r\n\r\n- tester donc des cartogrammes en utilisant des données venant du corpus web\r\n\r\n- attention comme toujours à choisir une projection pertinente\r\n\r\n- un des enjeux consiste à ne pas chercher à niveler les différents niveaux administratif (villes, régions, pays)\r\n\r\nSi l'on s'intéresse aux noms de lieux cités dans un site web, laisser les énonciations telles qu'elles et représenter les différents niveau indépendamment en fonction de leur importance (nombre d'occurrence). (le service de nom de lieux comme geonames.org a été décrit)\r\n\r\n- le cas des lieux imaginaires ou mythologiques (May Houa) : C.F. wikipedia atlantis \"category: mythological places\"\r\n\r\n3- CONTENUS : au delà du lien\r\n\r\nComment dépasser la seule analyse topologique du graphe des liens hypertextes ?\r\n\r\nComment y ajouter des couches d'analyses de contenus : textes, images, méta-données (sur les sites, pages ou liens), données structurées venant de réseaux sociaux…\r\n\r\n# les liens hypertexte forment le squelette du corpus :\r\n\r\nle logiciel HCI permettra de créer et stocker, stabiliser un corpus.\r\n\r\nEn plus des liens ce logiciel captera les contenus textes par défaut.\r\n\r\nL'agrégation qu'il permettra des pages au niveau web entités (voir méthode) offrira la possibilité de récupérer et analyser les contenus textes.\r\n\r\nOn pourra alors imaginer ajouter à l'analyse réseau hypertextes, une analyse sémantique.\r\n\r\nDe la même manière à moyen terme on pourrait imaginer ajouter des couches d'extraction puis traitement de contenus spécifiques :\r\n\r\n- extraction et reconnaissance d'images\r\n\r\n- extraction de twitter\r\n\r\n- extraction de données Facebook\r\n\r\nCes extraction pourront être traitées dans la même structure mémoire en tant qu'entités propre (non web). On pourra alors imaginer déclarer des liens entre entités :\r\n\r\n- lien entre une image et toutes les pages web qui la contiennent\r\n\r\n- lien entre une page web et tous les tweet qui la citent\r\n\r\n- lien entre un profile facebook et toutes les pages qu'il a liké…\r\n\r\nCe travail demandera la programmation de moteur d'extraction spéficique en fonction des plateformes. Pour un futur à moyen terme.\r\n\r\n4- METHODES : exploiter les degrés de libertés pour répondre à une question de recherche\r\n\r\n# interactions avec les terrains classiques\r\n\r\ndans les deux sens :\r\n\r\nweb vers terrain : utiliser son corpus web pour préparer une étude de terrain, retourner sur le terrain pour confronter le miroir déformant qu'est le web à la réalité des acteurs\r\n\r\nterrain vers web : compléter une étude (quanti ou quali) par une étude web de la même question\r\n\r\ncomplémentaire que si on sait \"faire avec\" le web. N'oubions pas que le web existe !\r\n\r\nexemple : il existe les groupes sociaux qui souhaitent rester invisibles sur le web (pour protection de la vie privée Facebook ou pour garantir la clandestinité).\r\n\r\nPour ces cas il faut avoir recours à la méthode bien connue des ethnologues : l'observation participative !\r\n\r\nC'est en prenant par au groupe que vous pourrez l'observer.\r\n\r\n# granularité : logique de site contre logique de pages\r\n\r\nl'outils HCI est basé sur ce principe. Chaque chercheur pourra définir la juste granulité à utiliser pour chaque entité web qu'il souhaite étudier (voir : http://jiminy.medialab.sciences-po.fr/hci/index.php/Web_entities)\r\n\r\nOn notera qu'il est théoriquement possible de créer des règles d'agrégation différentes sur un même corpus de pages web récoltés afin de permettre à différent chercheurs d'agréger/sélectionner les entités en fonction de leur question de recherche (voir http://cartonomics.org/2012/05/27/de-la-replicabilite-des-corpus-de-sites-web/)\r\n\r\n# sélection : comment sélectionner des points d'entrées, étendre le corpus et éviter l'aspiration ver la couche haute\r\n\r\n2 chose sont indispensables à assurer de bons critères de sélections des entités web à inclure ou exclure au corpus :\r\n\r\n- ré-expliciter la méthode et assurer un accompagnement des chercheurs à ce travail qui demande de bien comprendre la structure du web (service fourni par l'équipement d'excellence DIME-SHS/web)\r\n\r\n- une meilleur interaction entre décision du chercheur et l'automatisme du crawler (problème réglé par HCI) (voir sur ce potin le résumé écrit par JC Plantin de la session \"analyse comparée http://cartonomics.org/2012/05/27/de-la-replicabilite-des-corpus-de-sites-web/)\r\n\r\n# Open the black box\r\n\r\nComme pour toute méthode, il est indispensable que les chercheurs comprennent les enjeux des données.\r\n\r\nLes logiciels que nous créer doivent donc :\r\n\r\n- être open source pour garantir la lecture critique du code produit (voir doi:10.1038/nature10836 http://www.nature.com/nature/journal/v482/n7386/full/nature10836.html)\r\n\r\n- toujours donner à voir aux chercheurs les conséquences de ses décisions en terme de \"qualité\" des données\r\n\r\n- assurer un historique aussi complet que possible qui permettent de retracer la chaîne de décisions/traitements appliqués aux données\r\n\r\n# méthode exploratoire : \"to help the researcher construct the research question\"\r\n\r\nLa méthode de construction et d'analyse de corpus web est une méthode exploratoire.\r\n\r\nElle permet en grande partie de construire ou plutôt de faire évoluer une question de recherche.\r\n\r\nElle demande plusieurs itérations avant d'aboutir à un corpus qui soit adapté au positionnement du chercheur.\r\n\r\nLa classification set un bon exemple de ce fait. Elle est construite en aller retour entre récolte et analyse.\r\n\r\nDes efforts sont à mener pour permettre à un groupe de chercheur de comparer leurs corpus sur la base d'éléments de classification commun.\r\n\r\nCet enjeux dépasse le cadre restraint de la méthode web.\r\n\r\nNotre approche privilégiée au médialab serait de mêler le bottom-up au top-down en permettant au chercheur de créer librement sa grille de qualification tout en y incluant des autorités de vocabulaire contrôlé.\r\n\r\nCe serait un bon premier pas vers la comparabilité des corpus web (à ce sujet voir http://cartonomics.org/2012/05/27/de-la-replicabilite-des-corpus-de-sites-web/)\r\n\r\n# scalabilité : aller-retour avec de grand corpus non thématiques\r\n\r\nL'intention de départ qui a motivée la création d'outils de web mining spécialisé pour les chercheurs en Sciences sociales, mettait la priorité à la qualité devant la quantité.\r\n\r\nNous ciblons des corpus de petites tailles (relativement à l'échelle du web).\r\n\r\nCela étant Anat Ben-David a soulevé l'intérêt que pourrait représenter des corpus de plus grandes échelles qui permettraient de donner un contexte au corpus raffinés par le chercheur.\r\n\r\nDeux enjeux se cachent derrière cette intention :\r\n\r\n- Qualité et robustesse d'un outils capable de passer à l'échelle (passage de l'expérimentation à la création de logiciels professionnels)\r\n\r\n- de même les critères de sélection de cette plus grande échelle reste à être déterminés (follow the medium, TLD, en utilisant les index de grand moteur de recherche ?)\r\n\r\nLe logiciel HCI permettra de régler le niveau du compromis entre qualité et quantité.\r\n\r\nOn pourra choisir de récolter un grand nombre d'urls et les liens qu'ils échangent à condition de limiter la profondeur des détails au sein de ses entités ou bien se limiter dans le nombre d'URL afin de permettre une grande qualité de détails dans chacune d'elles. On pourra même mettre des exceptions de précisions qui permettront de sortir une page (ou un ensemble) précise comme une entité très profondément d'une entité web non détaillée suivant la règle classique (voir : http://jiminy.medialab.sciences-po.fr/hci/index.php/Precision_limit)\r\n\r\n# analyse visuelle : explorer, cartographier des questions\r\n\r\nL'analyse visuelle des corpus web est également un point méthodologique en développement.\r\n\r\nDeux difficultés :\r\n\r\n- les sciences sociales n'ont pas la tradition de l'image scientifique comme l'ont développée les sciences naturelles\r\n\r\n- exception faite de la raison graphique de Bertin qu'il faut ré-interpréter à l'heure des réseaux, de la dynamique de l'intéraction/exploration\r\n\r\nIl s'agit de donner les moyens au chercheur d'explorer (GEPHI) puis de cartographier (TubeMyNet en cours développement) son corpus.\r\n\r\nIl est important d'abandonner l'idée qu'une carte suffira à synthétiser le travail effectué.\r\n\r\nLes résultats d'un travail sur corpus web doivent retracer la constitution, l'exploration et l'analyse du corpus sous ces différentes facettes.\r\n\r\nCe n'est pas une carte mais bien des cartes, des récits, des visualisations qui pourront former l'ensemble suffisant à rendre compte du cheminement du chercheur."
  },
  "description": {
    "en": "",
    "fr": ""
  },
  "draft": true,
  "id": "fc64c8d3-01fa-4ce5-a5fc-62dc6d28e06d",
  "internal": false,
  "lastUpdated": 1357724820000,
  "oldSlug": "web-mining-pour-les-chercheurs-en-sciences-sociales",
  "people": [
    "66d33f76-f197-494f-b8c3-1a61d1f60655"
  ],
  "productions": [
  ],
  "slugs": [
    "web-mining-pour-les-chercheurs-en"
  ],
  "startDate": "2013-01-9",
  "title": {
    "en": "Web mining pour les chercheurs en Sciences Sociales",
    "fr": "Web mining pour les chercheurs en Sciences Sociales"
  },
  "type": "notice"
}